import cv2
import cv2.aruco as aruco
import numpy as np
import threading
from collections import deque
import math
import rclpy
import tf_transformations
from tf_transformations import quaternion_from_euler
from scipy.spatial.transform import Rotation
from ament_index_python.packages import get_package_share_directory
import sys
import signal, os


pkg_root = os.path.dirname(os.path.dirname(__file__))
scripts_dir = os.path.join(pkg_root, 'scripts')
sys.path.insert(0, scripts_dir)
from robot_interface_client import RobotInterfaceClient




def run_aruco_detector(stop_event, shared_data, robot):
    # openCV & ArUco_marker initialization
    cv2.utils.logging.setLogLevel(cv2.utils.logging.LOG_LEVEL_ERROR)
    cap = cv2.VideoCapture('/dev/camera_c270')

    # url = "http://192.168.0.33:5000/video_feed" # http://localhost:5000/video_feed
    # cap = cv2.VideoCapture(url, cv2.CAP_FFMPEG)
    robot.get_logger().info(f"isOpened={cap.isOpened()}")

    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)
    parameters = aruco.DetectorParameters()
    script_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(script_dir)
    calib_path = os.path.join(parent_dir, 'config', 'calib_data.npz')
    data = np.load(calib_path)
    camera_matrix = data['mtx']
    dist_coeffs = data['dist']
    # set size of Marker
    marker_length = 0.025
    robot.get_logger().info("ğŸ“¸ ArUco Detector Thread Started (ESC or Ctrl+C to exit)")

    # Hand-Eye Calibration Param T(câ†’g)
    R_cam2gripper = np.array([
        [-0.09837893,  0.16064060,  0.98209785],
        [-0.99123926, -0.10321322, -0.08241218],
        [ 0.08812674, -0.98160156,  0.16938727]
    ])

    t_cam2gripper = np.array([
        -0.05113446,
        -0.00675610,
        0.04876112
    ]).reshape(3, 1)
    
    T_cam2gripper = np.eye(4)
    T_cam2gripper[:3, :3] = R_cam2gripper
    T_cam2gripper[:3,  3] = t_cam2gripper.reshape(3)
    
    # === Main loop ===
    while not stop_event.is_set():
        ret, frame = cap.read()
        if not ret:
            continue
        
        frame = np.ascontiguousarray(frame)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)
        corners, ids, _ = detector.detectMarkers(gray)
 
        if ids is not None:
            rvecs = []
            tvecs = []

            # ArUco marker 3D ê¸°ì¤€ì  (marker_length ê¸°ì¤€)
            objp = np.array([
                [-marker_length/2,  marker_length/2, 0],
                [ marker_length/2,  marker_length/2, 0],
                [ marker_length/2, -marker_length/2, 0],
                [-marker_length/2, -marker_length/2, 0],
            ], dtype=np.float32)
            
            for i, corner in enumerate(corners):
                img_points = corner.reshape(4, 2).astype(np.float32)

                success, rvec, tvec = cv2.solvePnP(
                    objp,
                    img_points,
                    camera_matrix,
                    dist_coeffs,
                    flags=cv2.SOLVEPNP_IPPE_SQUARE
                )

                if success:
                    rvecs.append(rvec)
                    tvecs.append(tvec)

            rvecs = np.array(rvecs)
            tvecs = np.array(tvecs)

            for i in range(len(ids)):
                aruco.drawDetectedMarkers(frame, corners, ids)
                cv2.drawFrameAxes(frame, camera_matrix, dist_coeffs, rvecs[i], tvecs[i], marker_length)

                # Tmarker2cam
                rvec, tvec = rvecs[i], tvecs[i]       # 3x1, 3x1
                t_target2cam = tvec.reshape(3, 1)
                R_target2cam, _ = cv2.Rodrigues(rvec) # 3x3

                T_target2cam = np.eye(4)
                T_target2cam[:3, :3] = R_target2cam
                T_target2cam[:3,  3] = t_target2cam.reshape(3)

                # Tgripper2Base
                pose = robot.current_position
                orient = robot.current_orientation  # Quaternian (qx, qy, qz, qw)

                if orient is None or len(orient) != 4:
                    continue

                R_gripper2base = Rotation.from_quat(orient).as_matrix()
                T_gripper2base = np.eye(4)
                T_gripper2base[:3, :3] = R_gripper2base
                T_gripper2base[:3,  3] = np.array(pose)

                # === coordinate change (Target to Base) ===
                T_cam2base = T_gripper2base @ T_cam2gripper
                T_target2base = T_cam2base @ T_target2cam
                
                bx, by, bz = T_target2base[:3, 3]
                R_base2target = T_target2base[:3, :3]
                quat = Rotation.from_matrix(R_base2target).as_quat()
                qx, qy, qz, qw = quat
                # r_euler = Rotation.from_matrix(R_base2target)
                # roll, pitch, yaw = r_euler.as_euler('xyz', degrees=True)

                ################# terminal ì •ë³´ ì¶œë ¥
                # robot.get_logger().info(f"ID {ids[i][0]} | X={bx:.3f} Y={by:.3f} Z={bz:.3f}")
                # robot.get_logger().info(f"{roll, pitch, yaw}")
                
                if shared_data["record_mode"]:
                    shared_data["positions"].append((bx, by, bz, qx, qy, qz, qw))

                # í™”ë©´ í‘œì‹œìš© í…ìŠ¤íŠ¸
                cX, cY = int(corners[i][0][0][0]), int(corners[i][0][0][1])
                cv2.putText(frame, f"ID:{ids[i][0]} X={bx:.3f}m Y={by:.3f}m Z={bz:.3f}m", (cX, cY - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        cv2.imshow("Aruco Detection", frame)
        key = cv2.waitKey(1) & 0xFF
        if key == 27:  # ESC to exit
            stop_event.set()  # ESC í‚¤ ëˆ„ë¥´ë©´ ìŠ¤ë ˆë“œ ì¢…ë£Œ
            cap.release()                # ì¹´ë©”ë¼ í•´ì œ
            cv2.destroyAllWindows()      # ëª¨ë“  OpenCV ì°½ ë‹«ê¸°
            os.kill(os.getpid(), signal.SIGINT)
            break
        elif key == 32 and not shared_data["record_mode"]: # space ëˆ„ë¥´ë©´ ê¸°ë¡ ì‹œì‘
            robot.get_logger().info("ğŸŸ¢ Recording marker position for 30 frames...")
            shared_data["positions"].clear()
            robot.get_logger().info("ğŸŸ¢ check point1")
            shared_data["record_mode"] = True
            robot.get_logger().info("ğŸŸ¢ check point2")

        # ê¸°ë¡ ì¤‘ì¼ ë•Œ í”„ë ˆì„ ìˆ˜ ì„¸ê¸°
        if shared_data["record_mode"]:
            if len(shared_data["positions"]) >= 30:
                shared_data["record_mode"] = False
                shared_data["trigger"] = True  # í‰ê·  ê³„ì‚° íŠ¸ë¦¬ê±°
                robot.get_logger().info(f"ğŸŸ¢ check point3, {shared_data['trigger'], shared_data['record_mode']}")

    cap.release()
    cv2.destroyAllWindows()



def main():
    rclpy.init()
    robot = RobotInterfaceClient()

    robot.get_logger().info("ğŸš€ Moving to initial pose: ground_2")
    success = robot.move_to_named_and_wait("ground_2")
    if not success:
        robot.get_logger().error("âŒ Failed to move to ground_2 at startup")
        return

    shared_data = {
        "positions": deque(maxlen=30),
        "trigger": False,
        "record_mode": False,
    }

    stop_event = threading.Event()
    aruco_thread = threading.Thread(target=run_aruco_detector, args=(stop_event, shared_data, robot))
    aruco_thread.start()

    try:
        while True:
            if shared_data["trigger"]:
                robot.get_logger().info("ğŸŸ¢ check point4")
                shared_data["trigger"] = False  # íŠ¸ë¦¬ê±° ì´ˆê¸°í™”

                if len(shared_data["positions"]) < 30:
                    robot.get_logger().info("âš ï¸ Not enough frames collected.")
                    continue

                # 30ê°œ ì¢Œí‘œì˜ í‰ê·  ê³„ì‚°
                xs, ys, zs, qx, qy, qz, qw = zip(*shared_data["positions"])
                mean_x, mean_y, mean_z = round(np.mean(xs), 3), round(np.mean(ys), 3), round(np.mean(zs), 3)
                mean_z = mean_z + float(0.01)
                yaw = math.atan2(mean_y, mean_x)
                pitch = math.pi / 2

                # 2ï¸âƒ£ EE orientation êµ¬ì„±
                #   Pitch = +Ï€/2 (ì§€ë©´ í–¥í•˜ê²Œ), `````````````````````````````````````````````````````````````````````````Yaw = ë§ˆì»¤ yaw ë°©í–¥ ì •ë ¬
                q_ee = tf_transformations.quaternion_from_euler(0, pitch, yaw)
                
                # ë¡œë´‡ ì´ë™ ëª…ë ¹
                robot.get_logger().info(f"ğŸ¯ {mean_x}, {mean_y}, {mean_z}, {q_ee[0]:.3f}, {q_ee[1]:.3f}, {q_ee[2]:.3f}, {q_ee[3]:.3f}")
                robot.get_logger().info("ğŸŸ¢ check point5")

                robot.gripper_and_wait(0.019)
                success = robot.move_to_pose_and_wait(mean_x, mean_y, mean_z, q_ee[0], q_ee[1], q_ee[2], q_ee[3])
                if not success:
                    robot.get_logger().error("âŒ MoveToPose failed â†’ sequence aborted")
                    shared_data["record_mode"] = False
                    shared_data["trigger"] = False
                    shared_data["positions"].clear()
                    continue   # ğŸ”¥ ì—¬ê¸°ì„œ ì‹œí€€ìŠ¤ ì¢…ë£Œ
                robot.gripper_and_wait(-0.004)
                robot.move_to_named_and_wait("pick_1")
                robot.move_to_named_and_wait("place_2")
                robot.move_to_named_and_wait("place_1")
                robot.gripper_and_wait(0.019)
                robot.move_to_named_and_wait("ground_2")

                # robot.call_move_to_pose(mean_x, mean_y, mean_z, 0, 0, 0, 1)

                

            rclpy.spin_once(robot, timeout_sec=0.1)

    except KeyboardInterrupt:
        robot.get_logger().info("ğŸ›‘ Program interrupted by user (ESC OR Ctrl+C)")

    stop_event.set()
    robot.get_logger().info("âœ… All service tests completed.")
    aruco_thread.join()
    rclpy.shutdown()


if __name__ == '__main__':
    main()